\documentclass[letterpaper]{article}
\usepackage{natbib,alifexi}
\usepackage{todonotes}
\usepackage{hyperref}

\title{Implementation\\Playing FPS Games with Deep Reinforcement Learning}
\author{Titouan Christophe$^{1}$, Florentin Hennecker$^{2}$, Nikita Marchant$^{1}$ \and Bruno Rocha Pereira$^{1}$ \\
\mbox{}\\
$^1$Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium \\
$^2$Universit\'e Libre de Bruxelles, Boulevard du Triomphe - CP 212, 1050
Brussels, Belgium \\
\{tichrist,nimarcha,brochape\}@vub.ac.be, \{fhenneck\}@ulb.ac.be}


\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\todo{\citep{mnih2013playing} and blabla have full knowledge of the scene(Can use regular DQN), not in 3D FPS like doom -> need DQRN}
Deep Reinforcement learning has brought in the past solutions to automating and learning typical human-computer interactions. \todo{refs!}
However, most of the previous applications of Deep Q-Networks assume a full knowledge of their environment. The application of those traditional DQN is thus limited in the case of playing FPS\footnote{First Person Shooter} games, where the field of vision of the player is limited. Deep recurrent networks were proposed to solve this problem \citep{Hausknecht2015} applied to Atari game. This paper applies techniques used for 2D games and extends them for use on a 3D game named Doom.
In order to plug our work to the Doom game, we are using the Vizdoom API \citep{Kempka2016} that provides an easy use of the game controls as well as knowledge about the current state of the game.
The agents will be playing in a setup of deathmatch. This means that they play in a level that requires to have the maximum amount of kills and the minimum amount of deaths, which is called best k/d ratio.
\section{Methods}
In order to implement the techniques described in the paper,\todo{}

\subsection{Deep Recurrent Q-Networks}
\todo{partially observable environments}
\todo{estimates Q(o t , h tâˆ’1 , a t ) instead of Q(s t , a t )}

\subsection{Game Features}
As described in the paper \citep{Lample2016}, agents not using the game features were unable to accurately detect enemies and were thus randomly shooting around hoping that those shots would reach their targets. In order to overcome the gaps in efficiency, we used the game features. The \texttt{Label buffer} was used to fetch all the entities present in the vision frustum. However, entities hidden by other scene elements, typically walls, were also parts of that buffer. We then used the map characteristics in order to to check if an entity was visible or not. Even though this information can't be used during testing phase, it was used during the training of the agents to improve their accuracy.


\subsection{Hyperparameters}
Analogously to \cite{Mnih2015} and \cite{Lample2016}, we use a replay memory size of 1000000 frames. In the bootstrapping phase, our agent only plays random actions drawn from the set \texttt{TURN\_LEFT, MOVE\_LEFT, MOVE\_FORWARD, JUMP, MOVE\_RIGHT}. We do not add the \texttt{TURN\_RIGHT} and \texttt{MOVE\_BACKWARD} actions, because a uniformely random sample of all these actions would lead the agent to stay at the same place and face approximately the same direction all the time. This bootstrapping phase is used to populate the replay memory.

\todo{screen resolution}

\section{Results}
\begin{table}[h]
\centering
\begin{tabular}{cc}
\multicolumn{2}{c}{Test Setup}                         \\ \hline
\multicolumn{1}{c|}{CPU} & Intel i7-6800K 3.4 GHz      \\
\multicolumn{1}{c|}{GPU} & Nvidia GeForce GTX 1080 8Go \\
\multicolumn{1}{c|}{RAM} & 32 Go                      
\end{tabular}
\caption{Test setup hardware specification}
\label{tab:specs}
\end{table}

The training phase as well as the action phase were run on a high-end machine of which specifications are displayed in Table \ref{tab:specs}.

\section{Discussion}


\footnotesize
\bibliographystyle{apalike}
\bibliography{report}


\end{document}
