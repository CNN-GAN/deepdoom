@article{Kempka2016,
  author    = {Michal Kempka and
               Marek Wydmuch and
               Grzegorz Runc and
               Jakub Toczek and
               Wojciech Jaskowski},
  title     = {ViZDoom: {A} Doom-based {AI} Research Platform for Visual Reinforcement
               Learning},
  journal   = {CoRR},
  volume    = {abs/1605.02097},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.02097},
  timestamp = {Wed, 01 Jun 2016 15:51:07 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KempkaWRTJ16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{Mnih2015,
    abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
    archivePrefix = {arXiv},
    arxivId = {1312.5602},
    author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
    doi = {10.1038/nature14236},
    eprint = {1312.5602},
    file = {:home/titou/VUB/LearningDynamics/deepdoom/papers/mnih2015.pdf:pdf},
    isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
    issn = {0028-0836},
    journal = {Nature},
    mendeley-groups = {Project Learning Dynamics},
    number = {7540},
    pages = {529--533},
    pmid = {25719670},
    publisher = {Nature Publishing Group},
    title = {{Human-level control through deep reinforcement learning}},
    url = {http://dx.doi.org/10.1038/nature14236},
    volume = {518},
    year = {2015}
}


@article{Lample2016,
    abstract = {Advances in deep reinforcement learning have allowed autonomous agents to perform well on Atari games, often outperforming humans, using only raw pixels to make their decisions. However, most of these games take place in 2D environments that are fully observable to the agent. In this paper, we present the first architecture to tackle 3D environments in first-person shooter games, that involve partially observable states. Typically, deep reinforcement learning methods only utilize visual input for training. We present a method to augment these models to exploit game feature information such as the presence of enemies or items, during the training phase. Our model is trained to simultaneously learn these features along with minimizing a Q-learning objective, which is shown to dramatically improve the training speed and performance of our agent. Our architecture is also modularized to allow different models to be independently trained for different phases of the game. We show that the proposed architecture substantially outperforms built-in AI agents of the game as well as humans in deathmatch scenarios.},
    archivePrefix = {arXiv},
    arxivId = {1609.05521},
    author = {Lample, Guillaume and Chaplot, Devendra Singh},
    eprint = {1609.05521},
    number = {2015},
    title = {{Playing FPS Games with Deep Reinforcement Learning}},
    url = {http://arxiv.org/abs/1609.05521},
    year = {2016}
}


@article{Hausknecht2015,
    author    = {Matthew J. Hausknecht and
                 Peter Stone},
    title     = {Deep Recurrent Q-Learning for Partially Observable MDPs},
    journal   = {CoRR},
    volume    = {abs/1507.06527},
    year      = {2015},
    url       = {http://arxiv.org/abs/1507.06527},
    timestamp = {Sun, 02 Aug 2015 18:42:02 +0200},
    biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HausknechtS15},
    bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{Srivastava2014,
    abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
    archivePrefix = {arXiv},
    arxivId = {1102.4807},
    author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
    doi = {10.1214/12-AOS1000},
    eprint = {1102.4807},
    file = {:home/titou/VUB/LearningDynamics/deepdoom/papers/JMLRdropout.pdf:pdf},
    isbn = {1532-4435},
    issn = {15337928},
    journal = {Journal of Machine Learning Research},
    keywords = {deep learning,model combination,neural networks,regularization},
    mendeley-groups = {Project Learning Dynamics},
    pages = {1929--1958},
    title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
    volume = {15},
    year = {2014}
}

@misc{rmsprop, 
	title = {Neural Networks for Machine Learning, Lecture 6a},
	author = {Hinton, Geoff and Srivastava, Nitish and Swersky, Kevin},
	pages = {29},
	url = {http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
	year = {2012},
}

@article{Hornik1991,
    author = {Hornik, Kurt},
    title = {Approximation Capabilities of Multilayer Feedforward Networks},
    journal = {Neural Netw.},
    issue_date = {1991},
    volume = {4},
    number = {2},
    month = mar,
    year = {1991},
    issn = {0893-6080},
    pages = {251--257},
    numpages = {7},
    url = {http://dx.doi.org/10.1016/0893-6080(91)90009-T},
    doi = {10.1016/0893-6080(91)90009-T},
    acmid = {109700},
    publisher = {Elsevier Science Ltd.},
    address = {Oxford, UK, UK},
}

@misc{tensorflow,
	title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={http://tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
		Mart\'{\i}n~Abadi and
		Ashish~Agarwal and
		Paul~Barham and
		Eugene~Brevdo and
		Zhifeng~Chen and
		Craig~Citro and
		Greg~S.~Corrado and
		Andy~Davis and
		Jeffrey~Dean and
		Matthieu~Devin and
		Sanjay~Ghemawat and
		Ian~Goodfellow and
		Andrew~Harp and
		Geoffrey~Irving and
		Michael~Isard and
		Yangqing Jia and
		Rafal~Jozefowicz and
		Lukasz~Kaiser and
		Manjunath~Kudlur and
		Josh~Levenberg and
		Dan~Man\'{e} and
		Rajat~Monga and
		Sherry~Moore and
		Derek~Murray and
		Chris~Olah and
		Mike~Schuster and
		Jonathon~Shlens and
		Benoit~Steiner and
		Ilya~Sutskever and
		Kunal~Talwar and
		Paul~Tucker and
		Vincent~Vanhoucke and
		Vijay~Vasudevan and
		Fernanda~Vi\'{e}gas and
		Oriol~Vinyals and
		Pete~Warden and
		Martin~Wattenberg and
		Martin~Wicke and
		Yuan~Yu and
		Xiaoqiang~Zheng},
	  year={2015},
}
