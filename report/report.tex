\documentclass[letterpaper]{article}
\usepackage{natbib,alifexi}
\usepackage{todonotes}

\title{Implementation\\Playing FPS Games with Deep Reinforcement Learning}
\author{Titouan Christophe$^{1}$, Florentin Hennecker$^{2}$, Nikita Marchant$^{1}$ \and Bruno Rocha Pereira$^{1}$ \\
\mbox{}\\
$^1$Vrije Universiteit Brussel, Pleinlaan 2, 1050 Brussels, Belgium \\
$^2$Universit\'e Libre de Bruxelles, Boulevard du Triomphe - CP 212, 1050
Brussels, Belgium \\
\{tichrist,nimarcha,brochape\}@vub.ac.be, \{fhenneck\}@ulb.ac.be}


\begin{document}
\maketitle
\begin{abstract}
\end{abstract}

\section{Introduction}
\todo{\citep{mnih2013playing} and blabla have full knowledge of the scene(Can use regular DQN), not in 3D FPS like doom -> need DQRN}
\todo{Use of the Vizdoom API \citep{Kempka2016ViZDoom}}
\todo{Deathmap -> goal is to improve the k/d ratio}

\section{Methods}
In order to implement the techniques described in the paper,\todo{}

\subsection{Deep Recurrent Q-Networks}
\todo{partially observable environments}
\todo{estimates Q(o t , h tâˆ’1 , a t ) instead of Q(s t , a t )}

\subsection{Game Features}
As described in the paper \citep{Lample2016}, agents not using the game features were unable to accurately detect enemies and were thus randomly shooting around hoping that those shots would reach their targets. In order to overcome the gaps in efficiency, we used the game features. The \texttt{Label buffer} was used to fetch all the entities present in the vision frustum. However, entities hidden by other scene elements, typically walls, were also parts of that buffer. We then used the map characteristics in order to to check if an entity was visible or not. Even though this information can't be used during testing phase, it was used during the training of the agents to improve their accuracy.


\subsection{Hyperparameters}
Analogously to \cite{Mnih2015} and \cite{Lample2016}, we use a replay memory size of 1000000 frames. In the bootstrapping phase, our agent only plays random actions drawn from the set \texttt{TURN\_LEFT, MOVE\_LEFT, MOVE\_FORWARD, JUMP, MOVE\_RIGHT}. We do not add the \texttt{TURN\_RIGHT} and \texttt{MOVE\_BACKWARD} actions, because a uniformely random sample of all these actions would lead the agent to stay at the same place and face approximately the same direction all the time. This bootstrapping phase is used to populate the replay memory.

\todo{screen resolution}

\section{Results}

\section{Discussion}


\footnotesize
\bibliographystyle{apalike}
\bibliography{report}


\end{document}
